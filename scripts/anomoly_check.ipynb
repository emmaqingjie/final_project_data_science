{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from functools import reduce\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "given the source/checkin_checkout_history_updated.csv file, identify the users and its corresponding weeks where it has been acting differently/unusual\n",
    "steps:\n",
    "1. process the data to group by (user, week) and get {total visits, total length of visits, total calories burnt}\n",
    "    - can use map reduce to do this, or pandas <(user, week), (total visits, total length of visits, total calories burnt)>\n",
    "2. run isoloation forest on the data to identify the outliers for anomoly detection\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/_raw/checkin_checkout_history_updated.csv')\n",
    "df['checkin_time'] = pd.to_datetime(df['checkin_time'])\n",
    "df['checkout_time'] = pd.to_datetime(df['checkout_time'])\n",
    "df['session_seconds'] = (df['checkout_time'] - df['checkin_time']).dt.total_seconds()\n",
    "df['week'] = df['checkin_time'].dt.isocalendar().week\n",
    "# sometimes days in january counted as week 52 so I just changed them to week 0\n",
    "df.loc[(df['checkin_time'].dt.month == 1) & (df['week'] == 52), 'week'] = 0\n",
    "df.drop(['gym_id', 'checkin_time', 'checkout_time', 'workout_type'], axis=1, inplace=True)\n",
    "# lets us know the most recent week we have data from, useful for computing average weekly activity\n",
    "CONSTANT_LAST_WEEK = df['week'].max()\n",
    "CONSTANT_FIRST_WEEK = df['week'].min()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The next 2 functions are the basic map-reduce to transform the data into grouping by user/week\n",
    "# and counting how many times they visited in a week + total time + total calories burned\n",
    "\n",
    "# map each entry to key=id+week and value=numVisits,length,calories\n",
    "def mapFunc_groupByUserWeek(row):\n",
    "    _, row_data = row\n",
    "    user_id = row_data['user_id']\n",
    "    calories = int(row_data['calories_burned'])\n",
    "    seconds = int(row_data['session_seconds'])\n",
    "    week = int(row_data['week'])\n",
    "    return (str(user_id) + \"-\" + str(week), (1, seconds, calories))\n",
    "\n",
    "# reduce by summing, to calculate users gym activity by week\n",
    "def reduceFunc_groupByUserWeek(acc, pair):\n",
    "    key, value = pair\n",
    "    if key in acc:\n",
    "        acc[key] = tuple(map(lambda x, y: x + y, acc[key], value))\n",
    "    else:\n",
    "        # i think they're already ints after the changes I made above\n",
    "        acc[key] = (int(value[0]), int(value[1]), int(value[2]))\n",
    "    return acc   \n",
    "\n",
    "\n",
    "# if a user doesn't go in a whole week, we still need an empty entry for that week for our anomaly detection\n",
    "# so I'm going to find the first week each user started going to the gym so we can add every week after\n",
    "\n",
    "# map each entry to key=id and value=week\n",
    "def mapFunc_addEmptyWeeks(row):\n",
    "    _, entry = row\n",
    "    key = entry['user_id']\n",
    "    value = int(entry['week'])\n",
    "    return (key, value)\n",
    "\n",
    "# reduce by taking the first week they started going to the gym\n",
    "def reduceFunc_findFirstWeek(acc, pair):\n",
    "  \n",
    "    #TODO find first and last week. make the values a tuple of (first, last)\n",
    "    key, value = pair\n",
    "    if key in acc:\n",
    "        acc[key] = min(acc[key] , value)\n",
    "    else:\n",
    "        acc[key] = value\n",
    "    return acc\n",
    "\n",
    "def reduceFunc_findWeekRange(acc, pair):\n",
    "    key, value = pair\n",
    "    if key in acc:\n",
    "        newBot = min(acc[key][0] , value)\n",
    "        newTop = max(acc[key][1] , value)\n",
    "        acc[key] = (newBot, newTop)\n",
    "    else:\n",
    "        acc[key] = (value, value)\n",
    "    return acc\n",
    "\n",
    "# this reduce function will add all the empty weeks \n",
    "# and the output will be the starting dictionary inputted to reduceFunc_groupByUserWeek\n",
    "def reduceFunc_addEmptyWeeks(acc, pair):\n",
    "    user, week_range = pair\n",
    "    bot, top = week_range\n",
    "    for i in range(int(bot), int(top) + 1):\n",
    "        acc[user + '-' + str(i)] = (0, 0, 0)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up empty weeks first\n",
    "mappedData_addEmptyWeeks = list(map(mapFunc_addEmptyWeeks, df.iterrows()))\n",
    "\n",
    "userlyWeekRange = reduce(reduceFunc_findWeekRange, mappedData_addEmptyWeeks, {})\n",
    "emptyWeeks = reduce(reduceFunc_addEmptyWeeks, userlyWeekRange.items(), {})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now adding all the actual information to the \"empty weeks\" dict returned from above cell\n",
    "mappedData_groupByUserWeek = list(map(mapFunc_groupByUserWeek, df.iterrows()))\n",
    "\n",
    "weekly_userly_data = reduce(reduceFunc_groupByUserWeek, mappedData_groupByUserWeek, emptyWeeks)\n",
    "weekly_userly_data_dict = dict(weekly_userly_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv_df = pd.DataFrame.from_dict(reduced_data, orient='index', columns=['total_sessions', 'total_session_seconds', 'total_calories'])\n",
    "# # csv_df = csv_df.reset_index()\n",
    "\n",
    "# csv_df.to_csv('processed_data.csv', index=True)\n",
    "# csv_df\n",
    "\n",
    "# storing both dict version and other version NOTE: do not need to store in files as we can just use the variables\n",
    "# with open(\"groupByUserWeek.json\", \"w\") as file:\n",
    "#     json.dump(weekly_userly_data, file)\n",
    "\n",
    "# with open(\"groupByUserWeek_dict.json\", \"w\") as file:\n",
    "#     json.dump(weekly_userly_data_dict, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn jsons into a df, then run the anomoly forest on it\n",
    "\n",
    "transformed_data = [\n",
    "    (*key.split('-'), *values) for key, values in weekly_userly_data.items()\n",
    "]\n",
    "\n",
    "cleaned_dataframe = pd.DataFrame(transformed_data, columns=['user_id', 'week', 'total_sessions', 'total_session_seconds', 'total_calories'])\n",
    "cleaned_dataframe.set_index(['user_id', 'week'], inplace=True)\n",
    "\n",
    "cleaned_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_dataframe_with_anomoly = cleaned_dataframe.copy()\n",
    "model = IsolationForest(contamination=0.05, random_state=42)\n",
    "cleaned_dataframe_with_anomoly['anomaly'] = model.fit_predict(cleaned_dataframe_with_anomoly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after performing anomaly detection we want to see how relevent each anomaly is\n",
    "# going to use a modified TF.IDF where each document is a users list of anomaly weeks\n",
    "# and each word is a week, but instead of the week appearing a lot, we're going to use distance from mean\n",
    "# this will allow us to see how extreme of an anomaly it is, and if it's common among all users\n",
    "# perhaps everyones gym usage went down cause there was a holiday\n",
    "\n",
    "def mapFunc_findUserMean(row):\n",
    "    key_old,value_old = row\n",
    "    user, week = key_old.split('-')\n",
    "    #TODO add 2 versions of week to represent min and max\n",
    "    value_new = (int(week), value_old[0], value_old[1], value_old[2])\n",
    "    return (user, value_new)\n",
    "\n",
    "def reduceFunc_sumStats_minWeek(acc, pair):\n",
    "    key, value = pair\n",
    "    if key in acc:\n",
    "        #take minimum week to find when they started, sum rest of the values\n",
    "        #TODO also take max in the other part\n",
    "        acc[key] = ( min(acc[key][0] , value[0]) , \n",
    "                    max(acc[key][4] , value[0]),\n",
    "                    acc[key][2] + value[1] , \n",
    "                    acc[key][3] + value[2] , \n",
    "                    acc[key][4] + value[3] )\n",
    "    else:\n",
    "        acc[key] = (value[0], value[0], value[1], value[2], value[3])\n",
    "\n",
    "    return acc\n",
    "        \n",
    "def reduceFunc_averageStats(acc, pair):\n",
    "    key, value = pair\n",
    "    #TODO make value also contain \n",
    "    numWeeks = value[1] - value[0] + 1\n",
    "    acc[key] = (value[2] / numWeeks, value[3] / numWeeks, value[4] / numWeeks)\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mappedData_findUserMean = list(map(mapFunc_findUserMean, weekly_userly_data_dict.items()))\n",
    "\n",
    "weekly_userly_total = reduce(reduceFunc_sumStats_minWeek, mappedData_findUserMean, {})\n",
    "weekly_userly_mean = reduce(reduceFunc_averageStats, dict(weekly_userly_total).items(), {})\n",
    "\n",
    "weekly_userly_mean_dict = dict(weekly_userly_mean)\n",
    "\n",
    "#print(weekly_userly_mean_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"averageByUser.json\", \"w\") as file:\n",
    "    json.dump(weekly_userly_mean, file)\n",
    "\n",
    "with open(\"averageByUser_dict.json\", \"w\") as file:\n",
    "    json.dump(weekly_userly_mean_dict, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Okay for TF.IDF the TF part is supposed to be a number between 0-1 but distance from mean might not be\n",
    "# So I think I'm going to find the max distance from the minimum box containing the data and compute the \n",
    "# max distance from any vertex to mean and divide the actual distance by that\n",
    "\n",
    "# then we need to actually comput IDF for each week \n",
    "# which doesn't need to be repeated for each user since it's always the same\n",
    "\n",
    "# then we need to compute TF' for each user/week\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
