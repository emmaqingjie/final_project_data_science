{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from functools import reduce\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import json\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ngiven the source/checkin_checkout_history_updated.csv file, identify the users and its corresponding weeks where it has been acting differently/unusual\\nsteps:\\n1. process the data to group by (user, week) and get {total visits, total length of visits, total calories burnt}\\n    - can use map reduce to do this, or pandas <(user, week), (total visits, total length of visits, total calories burnt)>\\n2. run isoloation forest on the data to identify the outliers for anomoly detection\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "given the source/checkin_checkout_history_updated.csv file, identify the users and its corresponding weeks where it has been acting differently/unusual\n",
    "steps:\n",
    "1. process the data to group by (user, week) and get {total visits, total length of visits, total calories burnt}\n",
    "    - can use map reduce to do this, or pandas <(user, week), (total visits, total length of visits, total calories burnt)>\n",
    "2. run isoloation forest on the data to identify the outliers for anomoly detection\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_byVisit = pd.read_csv('../data/_raw/checkin_checkout_history_updated.csv')\n",
    "df_byVisit['checkin_time'] = pd.to_datetime(df_byVisit['checkin_time'])\n",
    "df_byVisit['checkout_time'] = pd.to_datetime(df_byVisit['checkout_time'])\n",
    "df_byVisit['session_seconds'] = (df_byVisit['checkout_time'] - df_byVisit['checkin_time']).dt.total_seconds()\n",
    "df_byVisit['week'] = df_byVisit['checkin_time'].dt.isocalendar().week\n",
    "# sometimes days in january counted as week 52 so I just changed them to week 0\n",
    "df_byVisit.loc[(df_byVisit['checkin_time'].dt.month == 1) & (df_byVisit['week'] == 52), 'week'] = 0\n",
    "df_byVisit.drop(['gym_id', 'checkin_time', 'checkout_time', 'workout_type'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONSTANT_TOTAL_USERS = df_byVisit['user_id'].nunique()\n",
    "\n",
    "CONSTANT_LAST_WEEK = df_byVisit['week'].max()\n",
    "CONSTANT_FIRST_WEEK = df_byVisit['week'].min()\n",
    "\n",
    "CONSTANT_AVG_CALORIES_SESSION = df_byVisit['calories_burned'].mean()\n",
    "CONSTANT_AVG_LENGTH_SESSION = df_byVisit['session_seconds'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The next 2 functions are the basic map-reduce to transform the data into grouping by user/week\n",
    "# and counting how many times they visited in a week + total time + total calories burned\n",
    "\n",
    "# map each entry to key=id+week and value=numVisits,length,calories\n",
    "def mapFunc_groupByUserWeek(row):\n",
    "    _, row_data = row\n",
    "    user_id = row_data['user_id']\n",
    "    calories = int(row_data['calories_burned'])\n",
    "    seconds = int(row_data['session_seconds'])\n",
    "    week = int(row_data['week'])\n",
    "    return (str(user_id) + \"-\" + str(week), (1, seconds, calories))\n",
    "\n",
    "# reduce by summing, to calculate users gym activity by week\n",
    "def reduceFunc_groupByUserWeek(acc, pair):\n",
    "    key, value = pair\n",
    "    if key in acc:\n",
    "        acc[key] = tuple(map(lambda x, y: x + y, acc[key], value))\n",
    "    else:\n",
    "        # i think they're already ints after the changes I made above\n",
    "        acc[key] = (int(value[0]), int(value[1]), int(value[2]))\n",
    "    return acc   \n",
    "\n",
    "\n",
    "# if a user doesn't go in a whole week, we still need an empty entry for that week for our anomaly detection\n",
    "# so I'm going to find the first week each user started going to the gym so we can add every week after\n",
    "\n",
    "# map each entry to key=id and value=week\n",
    "def mapFunc_addEmptyWeeks(row):\n",
    "    _, entry = row\n",
    "    key = entry['user_id']\n",
    "    value = int(entry['week'])\n",
    "    return (key, value)\n",
    "\n",
    "# reduce by taking the first week they started going to the gym\n",
    "def reduceFunc_findFirstWeek(acc, pair):\n",
    "  \n",
    "    #TODO find first and last week. make the values a tuple of (first, last)\n",
    "    key, value = pair\n",
    "    if key in acc:\n",
    "        acc[key] = min(acc[key] , value)\n",
    "    else:\n",
    "        acc[key] = value\n",
    "    return acc\n",
    "\n",
    "def reduceFunc_findWeekRange(acc, pair):\n",
    "    key, value = pair\n",
    "    if key in acc:\n",
    "        newBot = min(acc[key][0] , value)\n",
    "        newTop = max(acc[key][1] , value)\n",
    "        acc[key] = (newBot, newTop)\n",
    "    else:\n",
    "        acc[key] = (value, value)\n",
    "    return acc\n",
    "\n",
    "# this reduce function will add all the empty weeks \n",
    "# and the output will be the starting dictionary inputted to reduceFunc_groupByUserWeek\n",
    "def reduceFunc_addEmptyWeeks(acc, pair):\n",
    "    user, week_range = pair\n",
    "    bot, top = week_range\n",
    "    for i in range(int(bot), int(top) + 1):\n",
    "        acc[user + '-' + str(i)] = (0, 0, 0)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up empty weeks first\n",
    "mappedData_addEmptyWeeks = list(map(mapFunc_addEmptyWeeks, df_byVisit.iterrows()))\n",
    "\n",
    "userlyWeekRange = reduce(reduceFunc_findWeekRange, mappedData_addEmptyWeeks, {})\n",
    "emptyWeeks = reduce(reduceFunc_addEmptyWeeks, userlyWeekRange.items(), {})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now adding all the actual information to the \"empty weeks\" dict returned from above cell\n",
    "mappedData_groupByUserWeek = list(map(mapFunc_groupByUserWeek, df_byVisit.iterrows()))\n",
    "\n",
    "weekly_userly_data = reduce(reduceFunc_groupByUserWeek, mappedData_groupByUserWeek, emptyWeeks)\n",
    "weekly_userly_data_dict = dict(weekly_userly_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv_df = pd.DataFrame.from_dict(reduced_data, orient='index', columns=['total_sessions', 'total_session_seconds', 'total_calories'])\n",
    "# # csv_df = csv_df.reset_index()\n",
    "\n",
    "# csv_df.to_csv('processed_data.csv', index=True)\n",
    "# csv_df\n",
    "\n",
    "# storing both dict version and other version NOTE: do not need to store in files as we can just use the variables\n",
    "# with open(\"groupByUserWeek.json\", \"w\") as file:\n",
    "#     json.dump(weekly_userly_data, file)\n",
    "\n",
    "# with open(\"groupByUserWeek_dict.json\", \"w\") as file:\n",
    "#      json.dump(weekly_userly_data_dict, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn jsons into a df, then run the anomoly forest on it\n",
    "\n",
    "transformed_data = [\n",
    "    (*key.split('-'), *values) for key, values in weekly_userly_data.items()\n",
    "]\n",
    "\n",
    "df_byUserWeek_noIndex = pd.DataFrame(transformed_data, columns=['user_id', 'week', 'total_sessions', 'total_seconds', 'total_calories'])\n",
    "df_byUserWeek_noIndex.set_index(['user_id', 'week'], inplace=True)\n",
    "\n",
    "\n",
    "# ## TODO I think I did the same thing below, not sure which you like bette\n",
    "df_byUserWeek = pd.DataFrame(list(weekly_userly_data.items()), columns=[\"user_week\", \"visits_time_calories\"])\n",
    "\n",
    "df_byUserWeek[[\"user_id\", \"week\"]] = df_byUserWeek[\"user_week\"].str.split(\"-\", expand=True)\n",
    "df_byUserWeek[\"week\"] = df_byUserWeek[\"week\"].astype(int)\n",
    "\n",
    "df_byUserWeek[[\"total_sessions\", \"total_seconds\", \"total_calories\"]] = pd.DataFrame(df_byUserWeek[\"visits_time_calories\"].tolist(), index=df_byUserWeek.index)\n",
    "\n",
    "df_byUserWeek.drop(columns=[\"user_week\", \"visits_time_calories\"], inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# df_byUserWeek_noIndex.to_json(\"groupByUserDataFrame.json\", orient=\"records\", indent=4)\n",
    "\n",
    "CONSTANT_AVG_VISITS_WEEK = df_byUserWeek_noIndex['total_sessions'].mean()\n",
    "CONSTANT_AVG_CALORIES_WEEK = df_byUserWeek_noIndex['total_seconds'].mean()\n",
    "CONSTANT_AVG_LENGTH_WEEK = df_byUserWeek_noIndex['total_calories'].mean()\n",
    "\n",
    "# defines the distance between 2 points where the second point is supposed to be the mean or expected value\n",
    "# like the % distance away from point 2\n",
    "def distance(t1, v1, c1, point2):\n",
    "    v2, t2, c2 = point2\n",
    "    x = (v1-v2)/v2\n",
    "    y = (t1-t2)/t2\n",
    "    z = (c1-c2)/c2\n",
    "    return x**2+y**2+z**2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after performing anomaly detection we want to see how relevent each anomaly is\n",
    "# going to use a modified TF.IDF where each document is a users list of anomaly weeks\n",
    "# and each word is a week, but instead of the week appearing a lot, we're going to use distance from mean\n",
    "# this will allow us to see how extreme of an anomaly it is, and if it's common among all users\n",
    "# perhaps everyones gym usage went down cause there was a holiday\n",
    "\n",
    "def mapFunc_findUserMean(row):\n",
    "    key_old, value_old = row\n",
    "    user, week = key_old.split('-')\n",
    "    #TODO add 2 versions of week to represent min and max\n",
    "    value_new = (int(week), value_old[0], value_old[1], value_old[2])\n",
    "    return (user, value_new)\n",
    "\n",
    "def reduceFunc_sumStats_weekRange(acc, pair):\n",
    "    key, value = pair\n",
    "    if key in acc:\n",
    "        #take minimum week to find when they started, sum rest of the values\n",
    "        #TODO also take max in the other part\n",
    "        acc[key] = ( min(acc[key][0] , value[0]) , \n",
    "                    max(acc[key][1] , value[0]),\n",
    "                    acc[key][2] + value[1] , \n",
    "                    acc[key][3] + value[2] , \n",
    "                    acc[key][4] + value[3] )\n",
    "    else:\n",
    "        acc[key] = (value[0], value[0], value[1], value[2], value[3])\n",
    "\n",
    "    return acc\n",
    "        \n",
    "def reduceFunc_averageStats(acc, pair):\n",
    "    key, value = pair\n",
    "    #TODO make value also contain \n",
    "    numWeeks = value[1] - value[0] + 1\n",
    "    acc[key] = (value[2] / numWeeks, value[3] / numWeeks, value[4] / numWeeks)\n",
    "    return acc\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mappedData_findUserMean = list(map(mapFunc_findUserMean, weekly_userly_data_dict.items()))\n",
    "\n",
    "weekly_userly_total = reduce(reduceFunc_sumStats_weekRange, mappedData_findUserMean, {})\n",
    "weekly_userly_mean = reduce(reduceFunc_averageStats, weekly_userly_total.items(), {})\n",
    "\n",
    "weekly_userly_mean_dict = dict(weekly_userly_mean)\n",
    "\n",
    "#print(weekly_userly_mean_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Okay for TF.IDF the TF part is supposed to be a number between 0-1 but distance from mean might not be\n",
    "# So I think I'm going to find the max distance from the minimum box containing the data and compute the \n",
    "# max distance from any vertex to mean and divide the actual distance by that\n",
    "\n",
    "# then we need to actually comput IDF for each week \n",
    "# which doesn't need to be repeated for each user since it's always the same\n",
    "\n",
    "# then we need to compute TF' for each user/week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_byUserWeek[\"distance_from_mean\"] = df_byUserWeek.apply(\n",
    "    lambda row: distance(row[\"total_seconds\"], row[\"total_sessions\"], row[\"total_calories\"], weekly_userly_mean[row[\"user_id\"]]),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapFunc_findMaxDistance(row):\n",
    "    _, row_data = row\n",
    "    user_id = row_data['user_id']\n",
    "    distance = float(row_data['distance_from_mean'])\n",
    "    return (user_id, distance)\n",
    "\n",
    "# reduce by summing, to calculate users gym activity by week\n",
    "def reduceFunc_findMaxDistance(acc, pair):\n",
    "    key, value = pair\n",
    "    if key in acc:\n",
    "        acc[key] = max(acc[key], value)\n",
    "    else:\n",
    "        acc[key] = value\n",
    "    return acc \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run the above map reduce\n",
    "#also have dataframe modify distance row to divide by max distance per user\n",
    "mappedData_findMaxDistance = list(map(mapFunc_findMaxDistance, df_byUserWeek.iterrows()))\n",
    "user_max_distance = reduce(reduceFunc_findMaxDistance, mappedData_findMaxDistance, {})\n",
    "\n",
    "df_byUserWeek[\"distance_from_mean\"] = df_byUserWeek.apply(\n",
    "    lambda row: row[\"distance_from_mean\"] / user_max_distance[row[\"user_id\"]],\n",
    "    axis=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_byUserWeek_anomaly is the df with only anomolous data\n",
    "\n",
    "def mapFunc_IDF(row):\n",
    "    print(row)\n",
    "    return (1, 2)\n",
    "\n",
    "def reduceFunc_sumStats_weekRange(acc, pair):\n",
    "    key, value = pair\n",
    "    if key in acc:\n",
    "        #take minimum week to find when they started, sum rest of the values\n",
    "        #TODO also take max in the other part\n",
    "        acc[key] = ( min(acc[key][0] , value[0]) , \n",
    "                    max(acc[key][1] , value[0]),\n",
    "                    acc[key][2] + value[1] , \n",
    "                    acc[key][3] + value[2] , \n",
    "                    acc[key][4] + value[3] )\n",
    "    else:\n",
    "        acc[key] = (value[0], value[0], value[1], value[2], value[3])\n",
    "\n",
    "    return acc\n",
    "        \n",
    "def reduceFunc_averageStats(acc, pair):\n",
    "    key, value = pair\n",
    "    #TODO make value also contain \n",
    "    numWeeks = value[1] - value[0] + 1\n",
    "    acc[key] = (value[2] / numWeeks, value[3] / numWeeks, value[4] / numWeeks)\n",
    "    return acc\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "test = list(map(mapFunc_IDF, df_byUserWeek.iterrows()))\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_byUserWeek_noIndex_anomaly = df_byUserWeek_noIndex.copy()\n",
    "\n",
    "model = IsolationForest(contamination=0.05, random_state=42)\n",
    "\n",
    "df_byUserWeek_noIndex_anomaly['anomaly'] = model.fit_predict(df_byUserWeek_noIndex_anomaly)\n",
    "\n",
    "df_byUserWeek_noIndex_anomaly = df_byUserWeek_noIndex_anomaly[df_byUserWeek_noIndex_anomaly[\"anomaly\"] == -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"totalByUser.json\", \"w\") as file:\n",
    "#     json.dump(weekly_userly_total, file)\n",
    "\n",
    "\n",
    "# with open(\"averageByUser.json\", \"w\") as file:\n",
    "#     json.dump(weekly_userly_mean, file)\n",
    "\n",
    "# with open(\"averageByUser_dict.json\", \"w\") as file:\n",
    "#     json.dump(weekly_userly_mean_dict, file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
