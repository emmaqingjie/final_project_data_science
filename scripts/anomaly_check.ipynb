{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from functools import reduce\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "given the source/checkin_checkout_history_updated.csv file, identify the users and its corresponding weeks where it has been acting differently/unusual\n",
    "steps:\n",
    "1. process the data to group by (user, week) and get {total visits, total length of visits, total calories burnt}\n",
    "    - can use map reduce to do this, or pandas <(user, week), (total visits, total length of visits, total calories burnt)>\n",
    "2. run isoloation forest on the data to identify the outliers for anomaly detection\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple reading of data into a Pandas dataframe and changing a few of the columns to suit our needs\n",
    "df_byVisit = pd.read_csv('../data/_raw/checkin_checkout_history_updated.csv')\n",
    "#we decided not to do any anylsis based on time of day, so we needed length instead\n",
    "df_byVisit['checkin_time'] = pd.to_datetime(df_byVisit['checkin_time'])\n",
    "df_byVisit['checkout_time'] = pd.to_datetime(df_byVisit['checkout_time'])\n",
    "df_byVisit['session_seconds'] = (df_byVisit['checkout_time'] - df_byVisit['checkin_time']).dt.total_seconds()\n",
    "df_byVisit['week'] = df_byVisit['checkin_time'].dt.isocalendar().week\n",
    "# sometimes days in january counted as week 52 so I just changed them to week 0\n",
    "# if we had multiple years of data, we could just add 52*(current year - earliest year)\n",
    "df_byVisit.loc[(df_byVisit['checkin_time'].dt.month == 1) & (df_byVisit['week'] == 52), 'week'] = 0\n",
    "df_byVisit.drop(['gym_id', 'checkin_time', 'checkout_time', 'workout_type'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some self explanitory constants that we used\n",
    "# we might have also changed some things around to not use some of these constants\n",
    "CONSTANT_TOTAL_USERS = df_byVisit['user_id'].nunique()\n",
    "\n",
    "CONSTANT_LAST_WEEK = df_byVisit['week'].max()\n",
    "CONSTANT_FIRST_WEEK = df_byVisit['week'].min()\n",
    "\n",
    "CONSTANT_AVG_CALORIES_SESSION = df_byVisit['calories_burned'].mean()\n",
    "CONSTANT_AVG_LENGTH_SESSION = df_byVisit['session_seconds'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The next 2 functions are the basic map-reduce to transform the data into grouping by user/week\n",
    "# and counting how many times they visited in a week + total time + total calories burned\n",
    "\n",
    "# map each entry to key=id+week and value=numVisits,length,calories\n",
    "def mapFunc_groupByUserWeek(row):\n",
    "    _, row_data = row\n",
    "    user_id = row_data['user_id']\n",
    "    calories = int(row_data['calories_burned'])\n",
    "    seconds = int(row_data['session_seconds'])\n",
    "    week = int(row_data['week'])\n",
    "    # dealing with non-tuple keys is much easier especially for converting to files\n",
    "    return (str(user_id) + \"-\" + str(week), (1, seconds, calories))\n",
    "\n",
    "# reduce by summing, to calculate users gym activity by week\n",
    "def reduceFunc_groupByUserWeek(acc, pair):\n",
    "    key, value = pair\n",
    "    if key in acc:\n",
    "        acc[key] = tuple(map(lambda x, y: x + y, acc[key], value))\n",
    "    else:\n",
    "        # i think they're already ints after the changes I made above\n",
    "        acc[key] = (int(value[0]), int(value[1]), int(value[2]))\n",
    "    return acc   \n",
    "\n",
    "\n",
    "# if a user doesn't go in a whole week, we still need an empty entry for that week for our anomaly detection\n",
    "# so I'm going to find the first and last week each user went to fill in empty weeks\n",
    "\n",
    "# map each entry to key=id and value=week\n",
    "def mapFunc_addEmptyWeeks(row):\n",
    "    _, entry = row\n",
    "    key = entry['user_id']\n",
    "    value = int(entry['week'])\n",
    "    return (key, value)\n",
    "\n",
    "# reduce by taking the first and last week they started going to the gym\n",
    "def reduceFunc_findWeekRange(acc, pair):\n",
    "    key, value = pair\n",
    "    if key in acc:\n",
    "        newBot = min(acc[key][0] , value)\n",
    "        newTop = max(acc[key][1] , value)\n",
    "        acc[key] = (newBot, newTop)\n",
    "    else:\n",
    "        acc[key] = (value, value)\n",
    "    return acc\n",
    "\n",
    "# this reduce function will add all the empty weeks \n",
    "# and the output will be the starting dictionary inputted to reduceFunc_groupByUserWeek\n",
    "def reduceFunc_addEmptyWeeks(acc, pair):\n",
    "    user, week_range = pair\n",
    "    bot, top = week_range\n",
    "    for i in range(int(bot), int(top) + 1):\n",
    "        acc[user + '-' + str(i)] = (0, 0, 0)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up empty weeks first\n",
    "mappedData_addEmptyWeeks = list(map(mapFunc_addEmptyWeeks, df_byVisit.iterrows()))\n",
    "\n",
    "userlyWeekRange = reduce(reduceFunc_findWeekRange, mappedData_addEmptyWeeks, {})\n",
    "emptyWeeks = reduce(reduceFunc_addEmptyWeeks, userlyWeekRange.items(), {})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now adding all the actual information to the \"empty weeks\" dict returned from above cell\n",
    "mappedData_groupByUserWeek = list(map(mapFunc_groupByUserWeek, df_byVisit.iterrows()))\n",
    "\n",
    "weekly_userly_data = reduce(reduceFunc_groupByUserWeek, mappedData_groupByUserWeek, emptyWeeks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is much more necessary when the data is actually so large that it can't be stored in memory in its entirety\n",
    "\n",
    "\n",
    "# csv_df = pd.DataFrame.from_dict(reduced_data, orient='index', columns=['total_sessions', 'total_session_seconds', 'total_calories'])\n",
    "# # csv_df = csv_df.reset_index()\n",
    "\n",
    "# csv_df.to_csv('processed_data.csv', index=True)\n",
    "# csv_df\n",
    "\n",
    "# storing both dict version and other version NOTE: do not need to store in files as we can just use the variables\n",
    "# with open(\"groupByUserWeek.json\", \"w\") as file:\n",
    "#     json.dump(weekly_userly_data, file)\n",
    "\n",
    "# with open(\"groupByUserWeek_dict.json\", \"w\") as file:\n",
    "#      json.dump(weekly_userly_data_dict, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn jsons into a df, then run the anomaly forest on it\n",
    "\n",
    "transformed_data = [\n",
    "    (*key.split('-'), *values) for key, values in weekly_userly_data.items()\n",
    "]\n",
    "\n",
    "df_byUserWeek_noIndex = pd.DataFrame(transformed_data, columns=['user_id', 'week', 'total_sessions', 'total_seconds', 'total_calories'])\n",
    "df_byUserWeek_noIndex.set_index(['user_id', 'week'], inplace=True)\n",
    "\n",
    "\n",
    "# I think the code below does the same as the code above, \n",
    "# but having an index vs not having one was a little confusing when figuring out stuff later on\n",
    "df_byUserWeek = pd.DataFrame(list(weekly_userly_data.items()), columns=[\"user_week\", \"visits_time_calories\"])\n",
    "\n",
    "df_byUserWeek[[\"user_id\", \"week\"]] = df_byUserWeek[\"user_week\"].str.split(\"-\", expand=True)\n",
    "df_byUserWeek[\"week\"] = df_byUserWeek[\"week\"].astype(int)\n",
    "\n",
    "df_byUserWeek[[\"total_sessions\", \"total_seconds\", \"total_calories\"]] = pd.DataFrame(df_byUserWeek[\"visits_time_calories\"].tolist(), index=df_byUserWeek.index)\n",
    "\n",
    "df_byUserWeek.drop(columns=[\"user_week\", \"visits_time_calories\"], inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_byUserWeek_noIndex.to_json(\"groupByUserDataFrame.json\", orient=\"records\", indent=4)\n",
    "\n",
    "CONSTANT_AVG_VISITS_WEEK = df_byUserWeek_noIndex['total_sessions'].mean()\n",
    "CONSTANT_AVG_CALORIES_WEEK = df_byUserWeek_noIndex['total_seconds'].mean()\n",
    "CONSTANT_AVG_LENGTH_WEEK = df_byUserWeek_noIndex['total_calories'].mean()\n",
    "\n",
    "# defines the distance between 2 normalized points\n",
    "# point2 is the precomputed mean for a specific user and will already be prenormalized\n",
    "# the other values must be normalized using the mins and maxs in lst and the actual data v1 t1 c1\n",
    "\n",
    "def distance(v1, t1, c1, point2, lst, id):\n",
    "    minv, maxv, mint, maxt, minc, maxc = lst\n",
    "    # point2 should already be normalized\n",
    "    v2, t2, c2 = point2\n",
    "    \n",
    "    # normalize the data using min-max normalization\n",
    "    v3 = (v1 - minv) / (maxv - minv)\n",
    "    t3 = (t1 - mint) / (maxt - mint)\n",
    "    c3 = (c1 - minc) / (maxc - minc)\n",
    "    # This was purely for debugging \n",
    "    # if v3 > 1 or t3 > 1 or c3 > 1:\n",
    "    #     print('id: ' + id)\n",
    "    #     print(str(minv) + '-' + str(maxv))\n",
    "    #     print(str(mint) + '-' + str(maxt))\n",
    "    #     print(str(minc) + '-' + str(maxc))\n",
    "    #     print(str(v1) + '-' + str(t1) + '-' + str(c1))\n",
    "\n",
    "    return math.sqrt( (v2-v3)**2+(t2-t3)**2+(c2-c3)**2 )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after performing anomaly detection we want to see how relevent each anomaly is\n",
    "# going to use a modified TF.IDF where each document is a users list of anomaly weeks\n",
    "# and each word is a week, but instead of the week appearing a lot, we're going to use distance from mean\n",
    "# this will allow us to see how extreme of an anomaly it is, and if it's common among all users\n",
    "# perhaps everyones gym usage went down cause there was a holiday, or it was raining all week\n",
    "\n",
    "def mapFunc_findUserMean(row):\n",
    "    _, entry = row\n",
    "    value = (entry['week'], entry['total_sessions'], entry['total_seconds'], entry['total_calories'])\n",
    "    return (entry['user_id'], value)\n",
    "\n",
    "def reduceFunc_weekRange_sumStats_statRange(acc, pair):\n",
    "    key, value = pair\n",
    "    if key in acc:\n",
    "        # I know this is the most spaghetti code I've ever written but it's okay\n",
    "        # the first 2 entries in the tuple are to find each users first and last week\n",
    "        # the next 3 are to sum their visits, length, calories so I can find average\n",
    "        # the next 6 are to find min and max of each stat for my min-max normalization\n",
    "        acc[key] = ( min(acc[key][0] , value[0]) , \n",
    "                    max(acc[key][1] , value[0]),\n",
    "                    acc[key][2] + value[1] , \n",
    "                    acc[key][3] + value[2] , \n",
    "                    acc[key][4] + value[3] ,\n",
    "                    min(acc[key][5] , value[1]),\n",
    "                    max(acc[key][6] , value[1]),\n",
    "                    min(acc[key][7] , value[2]),\n",
    "                    max(acc[key][8] , value[2]),\n",
    "                    min(acc[key][9] , value[3]),\n",
    "                    max(acc[key][10] , value[3]) )\n",
    "    else:\n",
    "        acc[key] = (value[0], value[0], \n",
    "                    value[1], value[2], value[3], \n",
    "                    value[1], value[1], \n",
    "                    value[2], value[2], \n",
    "                    value[3], value[3])\n",
    "\n",
    "    return acc\n",
    "        \n",
    "def reduceFunc_normalizeAverageStats(acc, pair):\n",
    "    key, value = pair\n",
    "    numWeeks = value[1] - value[0] + 1\n",
    "\n",
    "    avgSession = value[2] / numWeeks\n",
    "    avgSeconds = value[3] / numWeeks\n",
    "    avgCalories = value[4] / numWeeks\n",
    "\n",
    "    # value[5:6] are min max of sessions\n",
    "    # value[7:8] are min max of seconds\n",
    "    # value[9:10] are min max of calories\n",
    "    normSession = (avgSession - value[5]) / (value[6] - value[5])\n",
    "    normSeconds = (avgSeconds - value[7]) / (value[8] - value[7])\n",
    "    normCalories = (avgCalories - value[9]) / (value[10] - value[9])\n",
    "    acc[key] = (normSession, normSeconds, normCalories)\n",
    "    return acc\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mappedData_findUserMean = list(map(mapFunc_findUserMean, df_byUserWeek.iterrows()))\n",
    "\n",
    "weekly_userly_total = reduce(reduceFunc_weekRange_sumStats_statRange, mappedData_findUserMean, {})\n",
    "weekly_userly_mean = reduce(reduceFunc_normalizeAverageStats, weekly_userly_total.items(), {})\n",
    "\n",
    "\n",
    "#print(weekly_userly_mean_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Okay for TF.IDF the TF part is supposed to be a number between 0-1 but distance from mean might not be,\n",
    "# and also a difference in 1 visit and 2 is much more significant that 3600 seconds and 3601\n",
    "\n",
    "# to combat this, I used min-max normalization specific to each user to define a more appropriate distance metric\n",
    "# then I found the max distance and divided each distance by that, user by user, so get the number between 0-1\n",
    "\n",
    "# then we need to actually compute IDF for each week \n",
    "# which doesn't need to be repeated for each user since it's always the same\n",
    "# \n",
    "\n",
    "# then we need to compute TF' for each user/week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this distance is normalized\n",
    "df_byUserWeek[\"distance_from_mean\"] = df_byUserWeek.apply(\n",
    "    lambda entry: distance(entry[\"total_sessions\"], entry[\"total_seconds\"], entry[\"total_calories\"], \n",
    "                         weekly_userly_mean[entry[\"user_id\"]],\n",
    "                         weekly_userly_total[entry[\"user_id\"]][5:11], entry['user_id']),\n",
    "    axis=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapFunc_findMaxDistance(row):\n",
    "    _, row_data = row\n",
    "    user_id = row_data['user_id']\n",
    "    distance = float(row_data['distance_from_mean'])\n",
    "    return (user_id, distance)\n",
    "\n",
    "# reduce by taking max\n",
    "def reduceFunc_findMaxDistance(acc, pair):\n",
    "    key, value = pair\n",
    "    if key in acc:\n",
    "        acc[key] = max(acc[key], value)\n",
    "    else:\n",
    "        acc[key] = value\n",
    "    return acc \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run the above map reduce\n",
    "#also have dataframe modify distance row to divide by max distance per user\n",
    "mappedData_findMaxDistance = list(map(mapFunc_findMaxDistance, df_byUserWeek.iterrows()))\n",
    "user_max_distance = reduce(reduceFunc_findMaxDistance, mappedData_findMaxDistance, {})\n",
    "\n",
    "# now this distance is between 0-1 like Term Frequency is supposed to be\n",
    "df_byUserWeek[\"distance_from_mean\"] = df_byUserWeek.apply(\n",
    "    lambda row: row[\"distance_from_mean\"] / user_max_distance[row[\"user_id\"]],\n",
    "    axis=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this process takes forever to run so we just run it once and save it into a file\n",
    "#if you really want to run this yourself, just change False to True but it might take 20 minutes\n",
    "if False:\n",
    "    df_byUserWeek_withanomaly  = df_byUserWeek_noIndex.copy()\n",
    "    df_byUserWeek_withanomaly.reset_index(inplace=True)\n",
    "\n",
    "    model = IsolationForest(contamination=0.05, random_state=42)\n",
    "\n",
    "    column_to_check = 0\n",
    "\n",
    "    def detect_anomalies_for_user(user_group):\n",
    "    \n",
    "        user_group['anomaly'] = model.fit_predict(user_group[['total_sessions', 'total_seconds', 'total_calories']])\n",
    "        \n",
    "        return user_group\n",
    "\n",
    "    grouped = list(df_byUserWeek_withanomaly.groupby('user_id'))\n",
    "\n",
    "    def process_group(group):\n",
    "        user_id, user_group = group\n",
    "        if len(user_group) < 2:\n",
    "            print(user_id)\n",
    "            return pd.DataFrame()\n",
    "        return detect_anomalies_for_user(user_group)\n",
    "        \n",
    "    results = map(process_group, grouped)\n",
    "    df_anomalies = pd.concat(results)\n",
    "    df_anomalies['week'] = df_anomalies['week'].astype('int64')\n",
    "    df_anomalies = df_anomalies.merge(df_byUserWeek[['user_id', 'week', 'distance_from_mean']], \n",
    "                     on=['user_id', 'week'], \n",
    "                     how='inner')\n",
    "    df_anomalies.to_csv('df_byUserWeek_withAnomaly.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapFunc_IDF(row):\n",
    "    _, entry = row\n",
    "    key = entry['week']\n",
    "    value = 1\n",
    "\n",
    "    return (key, value)\n",
    "\n",
    "def reduceFunc_countWeeks(acc, pair):\n",
    "    key, value = pair\n",
    "    if key in acc:\n",
    "        acc[key] += value\n",
    "    else:\n",
    "        acc[key] = (value)\n",
    "\n",
    "    return acc\n",
    "        \n",
    "def reduceFunc_computeIDF(acc, pair):\n",
    "    key, value = pair\n",
    "\n",
    "    idf = math.log(CONSTANT_TOTAL_USERS/value)\n",
    "    acc[key] = idf\n",
    "    return acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading back the anomalous data\n",
    "df_byUserWeek_all = pd.read_csv('df_byUserWeek_withAnomaly.csv')\n",
    "df_byUserWeek_anomaly = df_byUserWeek_all.copy()\n",
    "df_byUserWeek_anomaly = df_byUserWeek_all[df_byUserWeek_all['anomaly'] == -1]\n",
    "\n",
    "\n",
    "# given any week, enter that as the key to weekly_IDF and the value will be the necessary IDF\n",
    "# for computing TF.IDF\n",
    "mappedData_findingIDF = list(map(mapFunc_IDF, df_byUserWeek_anomaly.iterrows()))\n",
    "\n",
    "weekly_anomaly_count = reduce(reduceFunc_countWeeks, mappedData_findingIDF, {})\n",
    "weekly_IDF = reduce(reduceFunc_computeIDF, weekly_anomaly_count.items(), {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this adds a column to make the next map reduce easier\n",
    "df_byUserWeek_anomaly['IDF'] = df_byUserWeek_anomaly['week'].map(weekly_IDF)\n",
    "\n",
    "# while it shouldn't happen that any week in this dataframe doesn't have a corresponding entry in the dict\n",
    "# this is just to prevent any bugs\n",
    "df_byUserWeek_anomaly['IDF'] = df_byUserWeek_anomaly['IDF'].fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_byUserWeek_anomaly[\"TFIDF\"] = df_byUserWeek_anomaly['distance_from_mean'] * df_byUserWeek_anomaly['IDF']\n",
    "\n",
    "#TODO make a histogram for the values and use it to pick a nice cutoff\n",
    "# put that in the report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is for graphing purposes. if you wish to generate more graphs, turn False into True and alter the user_id below\n",
    "if False:\n",
    "    anomaly_map = {}\n",
    "    for user_id in df_byUserWeek_anomaly['user_id'].unique():\n",
    "        anomaly_count = len(df_byUserWeek_anomaly[(df_byUserWeek_anomaly['user_id'] == user_id) & \n",
    "                                                (df_byUserWeek_anomaly['anomaly'] == -1)])\n",
    "        if anomaly_count not in anomaly_map:\n",
    "            anomaly_map[anomaly_count] = []\n",
    "        if len(anomaly_map[anomaly_count]) < 10:\n",
    "            anomaly_map[anomaly_count].append(user_id)\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "    def plot_user_3d(df, user_id):\n",
    "        user_data = df[df['user_id'] == user_id]\n",
    "        \n",
    "        fig = plt.figure(figsize=(12, 8))\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        \n",
    "        normal = user_data[user_data['anomaly'] != -1]\n",
    "        ax.scatter(normal['total_sessions'], normal['total_seconds'], \n",
    "                normal['total_calories'], c='blue', label='Normal')\n",
    "        \n",
    "        anomalies = user_data[user_data['anomaly'] == -1]\n",
    "        ax.scatter(anomalies['total_sessions'], anomalies['total_seconds'], \n",
    "                anomalies['total_calories'], c='red', label='Anomaly')\n",
    "        \n",
    "        ax.set_xlabel('Total Sessions')\n",
    "        ax.set_ylabel('Total Seconds')\n",
    "        ax.set_zlabel('Total Calories')\n",
    "        ax.set_title(f'User {user_id} Activity Analysis')\n",
    "        ax.legend()\n",
    "        plt.savefig(f'user_{user_id}_3d_plot.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "    plot_user_3d(df_byUserWeek_all, user_id=\"user_1017\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
